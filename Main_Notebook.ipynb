{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler,EarlyStopping\n",
    "from tensorflow.keras.metrics import AUC,BinaryAccuracy \n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.models import load_model\n",
    "from Folds_creator.Creator import Fold_Creator\n",
    "from model.models import ModelCreation\n",
    "from TFData.Dataset import Dataset,Dataset_TTA\n",
    "from callbacks.CyclicLR import CyclicLR\n",
    "from utils import build_lrfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = './Dataset/train.csv'\n",
    "TEST_DF = './Dataset/test.csv'\n",
    "TEST_TFRECORDS = tf.io.gfile.glob('./Dataset/tfrecords/test*.tfrec')\n",
    "TRAINING_DATA_SIZE = 33126\n",
    "TEST_DATA_SIZE = 10982\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = [1024,1024]\n",
    "RESIZE_SHAPE = [256,256]\n",
    "EPOCHS = 5 \n",
    "SHUFFLE = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "N_SPLITS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "LR_MAX = 0.0003 \n",
    "LR_MIN = 0.00003 \n",
    "LR_RAMPUP_EPOCHS = 2\n",
    "LR_SUSTAIN_EPOCHS = 1\n",
    "LR_EXP_DECAY = 0.7\n",
    "SEED = 48\n",
    "scheduler = build_lrfn(lr_start=LEARNING_RATE,\n",
    "                        lr_max=LR_MAX,\n",
    "                        lr_min=LR_MIN,\n",
    "                        lr_rampup_epochs=LR_RAMPUP_EPOCHS,\n",
    "                        lr_sustain_epochs=LR_SUSTAIN_EPOCHS,\n",
    "                        lr_exp_decay=LR_EXP_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fold = Fold_Creator(train_df_path=TRAIN_DF,\n",
    "                        test_df_path=TEST_DF,\n",
    "                        tfrecord_path=None,\n",
    "                        fold_type='StratifiedGroupKFold',\n",
    "                        n_splits=N_SPLITS,shuffle=True,\n",
    "                        random_state=SEED,group_col='patient_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof_val_predictions = np.zeros((TRAINING_DATA_SIZE,))\n",
    "test_preds = np.zeros((TEST_DATA_SIZE,N_SPLITS))\n",
    "    \n",
    "i = 1\n",
    "    \n",
    "for trn_idx,val_idx,train_path,train_label,valid_path,valid_label in Fold.create_folds_generator():\n",
    "\n",
    "    tf.keras.backend.clear_session() # Clear session so the other model will be trained on new one.\n",
    "\n",
    "    print(\"=\"* 20,f'Fold_{i}',\"=\" * 20)\n",
    "\n",
    "    train = [train_path,train_label]\n",
    "    validation = [valid_path,valid_label]\n",
    "\n",
    "    save_path = f'./model/saved_models/EfficientNetB3_Non_Linear_LS_0.05_fold_{i}.h5'\n",
    "\n",
    "    print(f\"Save model in path: {save_path}\")\n",
    "\n",
    "    callbacks = [ \n",
    "        ModelCheckpoint(filepath=save_path,monitor='val_auc',verbose=1,save_best_only=True,mode='max'),\n",
    "        LearningRateScheduler(scheduler,verbose=1),\n",
    "    ]\n",
    "\n",
    "    Data = Dataset(\n",
    "        train_files = train,\n",
    "        test_files = TEST_TFRECORDS,\n",
    "        validation_files = validation,\n",
    "        validation_split = 0.2,\n",
    "        image_size = IMAGE_SIZE,\n",
    "        shuffle = SHUFFLE,\n",
    "        dataset_size = TRAINING_DATA_SIZE,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        resize_shape = RESIZE_SHAPE\n",
    "    )\n",
    "\n",
    "    Model = ModelCreation(\n",
    "        architecture = 'efficientnet',\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        input_shape = (*RESIZE_SHAPE,3),\n",
    "        output_shape = 1, \n",
    "        optimizer = Adam,\n",
    "        metric = [BinaryAccuracy(),AUC()],\n",
    "        loss = BinaryCrossentropy(label_smoothing=0.05),\n",
    "        mode = 'non_linear',\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    training = Data.get_train_from_tensor_slices()\n",
    "    validation_data = Data.get_val_from_tensor_slices()\n",
    "\n",
    "    history = Model.model.fit(training,\n",
    "                                steps_per_epoch = Data.get_train_steps_per_epoch(),\n",
    "                                epochs = EPOCHS,\n",
    "                                validation_data = validation_data,\n",
    "                                validation_steps = Data.get_validation_steps_per_epoch(),\n",
    "                                verbose = 1,\n",
    "                                callbacks = Model.inject_callbacks(callbacks))\n",
    "\n",
    "    print(f\"Loading model from path: {save_path}\")\n",
    "\n",
    "    Load_model = load_model(save_path)\n",
    "\n",
    "    validation_images = validation_data.map(lambda image,label: image)\n",
    "    \n",
    "    probabilities = Load_model.predict(validation_images)\n",
    "\n",
    "    oof_val_predictions[val_idx] = np.concatenate(probabilities)\n",
    "\n",
    "    #Get test set predictions in fold \n",
    "\n",
    "    test_data = Data.get_test_dataset()\n",
    "\n",
    "    test_images = test_data.map(lambda image,idnum : image)\n",
    "    test_probabilities = Load_model.predict(test_images)\n",
    "\n",
    "    test_preds[:,i-1] = np.concatenate(test_probabilities)\n",
    "\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = './Dataset/train.csv'\n",
    "TEST = './Dataset/test.csv'\n",
    "TRAIN_DF = tf.io.gfile.glob('./Dataset/CDeotte/Melanoma2020_128x128/train*.tfrec')\n",
    "TRAIN_DF_2 = tf.io.gfile.glob('./Dataset/CDeotte/Melanoma2019_128x128/train*.tfrec')\n",
    "TEST_TFRECORDS = tf.io.gfile.glob('./Dataset/tfrecords/test*.tfrec')\n",
    "TRAINING_DATA = TRAIN_DF + TRAIN_DF_2\n",
    "TRAINING_DATA_SIZE = 58457\n",
    "TEST_DATA_SIZE = 10982\n",
    "BATCH_SIZE = 8\n",
    "IMAGE_SIZE = [128,128]\n",
    "RESIZE_SHAPE = [128,128]\n",
    "INPUT_SHAPES = ((*RESIZE_SHAPE,3),(4,))\n",
    "EPOCHS = 5 \n",
    "SHUFFLE = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "N_SPLITS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "LR_MAX = 0.0003 \n",
    "LR_MIN = 0.00003 \n",
    "LR_RAMPUP_EPOCHS = 2\n",
    "LR_SUSTAIN_EPOCHS = 1\n",
    "LR_EXP_DECAY = 0.7\n",
    "SEED = 48\n",
    "scheduler = build_lrfn(lr_start=LEARNING_RATE,\n",
    "                        lr_max=LR_MAX,\n",
    "                        lr_min=LR_MIN,\n",
    "                        lr_rampup_epochs=LR_RAMPUP_EPOCHS,\n",
    "                        lr_sustain_epochs=LR_SUSTAIN_EPOCHS,\n",
    "                        lr_exp_decay=LR_EXP_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fold = Fold_Creator(train_df_path=TRAIN,\n",
    "                        test_df_path=TEST,\n",
    "                        tfrecord_path=TRAINING_DATA,\n",
    "                        fold_type='KFold',\n",
    "                        n_splits=N_SPLITS,shuffle=True,\n",
    "                        random_state=SEED,group_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "==================== Fold_1 ====================\nModel: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninp1 (InputLayer)               [(None, 128, 128, 3) 0                                            \n__________________________________________________________________________________________________\nefficientnet-b3 (Model)         (None, 4, 4, 1536)   10783528    inp1[0][0]                       \n__________________________________________________________________________________________________\nglobal_average_pooling2d (Globa (None, 1536)         0           efficientnet-b3[1][0]            \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 2048)         3147776     global_average_pooling2d[0][0]   \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 2048)         8192        dense[0][0]                      \n__________________________________________________________________________________________________\ninp2 (InputLayer)               [(None, 4)]          0                                            \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1000)         2049000     batch_normalization[0][0]        \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 128)          640         inp2[0][0]                       \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 1000)         4000        dense_2[0][0]                    \n__________________________________________________________________________________________________\nleaky_re_lu (LeakyReLU)         (None, 128)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 300)          300300      batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 128)          512         leaky_re_lu[0][0]                \n__________________________________________________________________________________________________\nbatch_normalization_3 (BatchNor (None, 300)          1200        dense_3[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 2176)         0           batch_normalization[0][0]        \n                                                                 batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 80)           24080       batch_normalization_3[0][0]      \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 1300)         2830100     concatenate[0][0]                \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 60)           4860        dense_4[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_6 (BatchNor (None, 1300)         5200        dense_7[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_4 (BatchNor (None, 60)           240         dense_5[0][0]                    \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 400)          520400      batch_normalization_6[0][0]      \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 20)           1220        batch_normalization_4[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_7 (BatchNor (None, 400)          1600        dense_8[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_5 (BatchNor (None, 20)           80          dense_6[0][0]                    \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 70)           28070       batch_normalization_7[0][0]      \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 170)          0           dense_4[0][0]                    \n                                                                 batch_normalization_5[0][0]      \n                                                                 dense_9[0][0]                    \n__________________________________________________________________________________________________\ndense_10 (Dense)                (None, 20)           3420        concatenate_1[0][0]              \n__________________________________________________________________________________________________\nbatch_normalization_8 (BatchNor (None, 20)           80          dense_10[0][0]                   \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 1)            21          batch_normalization_8[0][0]      \n==================================================================================================\nTotal params: 19,714,519\nTrainable params: 19,616,671\nNon-trainable params: 97,848\n__________________________________________________________________________________________________\nCreated model: efficientnet\nInput Shape: ((128, 128, 3), (4,))\nOutput Shape: 1\nMixed precission is set\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 3e-05.\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 3e-05.\nEpoch 1/5\n5846/5846 [==============================] - ETA: 0s - loss: 0.5723 - binary_accuracy: 0.7552 - auc: 0.5980\nEpoch 00001: val_auc improved from -inf to 0.65294, saving model to ./model/saved_models/EfficientNetB3_Non_Linear_LS_0.05_fold_1.h5\n\nEpoch 00001: val_auc did not improve from 0.65294\n5846/5846 [==============================] - 2258s 386ms/step - loss: 0.5723 - binary_accuracy: 0.7552 - auc: 0.5980 - val_loss: 0.4419 - val_binary_accuracy: 0.8918 - val_auc: 0.6529 - lr: 3.0000e-05\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00016499999999999997.\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00016499999999999997.\nEpoch 2/5\n5846/5846 [==============================] - ETA: 0s - loss: 0.2773 - binary_accuracy: 0.9374 - auc: 0.8333\nEpoch 00002: val_auc did not improve from 0.65294\n\nEpoch 00002: val_auc did not improve from 0.65294\n5846/5846 [==============================] - 2252s 385ms/step - loss: 0.2773 - binary_accuracy: 0.9374 - auc: 0.8333 - val_loss: 0.4972 - val_binary_accuracy: 0.8567 - val_auc: 0.2512 - lr: 1.6500e-04\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.0003.\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.0003.\nEpoch 3/5\n5846/5846 [==============================] - ETA: 0s - loss: 0.2418 - binary_accuracy: 0.9418 - auc: 0.9199\nEpoch 00003: val_auc improved from 0.65294 to 0.81747, saving model to ./model/saved_models/EfficientNetB3_Non_Linear_LS_0.05_fold_1.h5\n\nEpoch 00003: val_auc did not improve from 0.81747\n5846/5846 [==============================] - 2238s 383ms/step - loss: 0.2418 - binary_accuracy: 0.9418 - auc: 0.9199 - val_loss: 0.3218 - val_binary_accuracy: 0.9122 - val_auc: 0.8175 - lr: 3.0000e-04\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.0003.\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.0003.\nEpoch 4/5\n5846/5846 [==============================] - ETA: 0s - loss: 0.2292 - binary_accuracy: 0.9476 - auc: 0.9357\nEpoch 00004: val_auc did not improve from 0.81747\n\nEpoch 00004: val_auc did not improve from 0.81747\n5846/5846 [==============================] - 2243s 384ms/step - loss: 0.2292 - binary_accuracy: 0.9476 - auc: 0.9357 - val_loss: 0.7082 - val_binary_accuracy: 0.8595 - val_auc: 0.7260 - lr: 3.0000e-04\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.00021899999999999996.\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.00021899999999999996.\nEpoch 5/5\n5846/5846 [==============================] - ETA: 0s - loss: 0.2243 - binary_accuracy: 0.9503 - auc: 0.9428\nEpoch 00005: val_auc did not improve from 0.81747\n\nEpoch 00005: val_auc did not improve from 0.81747\n5846/5846 [==============================] - 2246s 384ms/step - loss: 0.2243 - binary_accuracy: 0.9503 - auc: 0.9428 - val_loss: 1.2691 - val_binary_accuracy: 0.8039 - val_auc: 0.7220 - lr: 2.1900e-04\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "`filenames` must be a `tf.data.Dataset` of `tf.string` elements.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b2756dbf23c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mdataset_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_full_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mimage_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_full\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Melanoma\\Melanoma-Kaggle\\TFData\\Dataset.py\u001b[0m in \u001b[0;36mget_full_dataset\u001b[1;34m(self, filenames)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_full_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_data_setup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Melanoma\\Melanoma-Kaggle\\TFData\\Dataset.py\u001b[0m in \u001b[0;36mload_full_dataset\u001b[1;34m(self, filenames)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \"\"\"\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_parallel_reads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_full_tfrecord\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filenames, compression_type, buffer_size, num_parallel_reads)\u001b[0m\n\u001b[0;32m    334\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0many\u001b[0m \u001b[0margument\u001b[0m \u001b[0mdoes\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \"\"\"\n\u001b[1;32m--> 336\u001b[1;33m     \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_or_validate_filenames_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\readers.py\u001b[0m in \u001b[0;36m_create_or_validate_filenames_dataset\u001b[1;34m(filenames)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_legacy_output_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m       raise TypeError(\n\u001b[1;32m---> 49\u001b[1;33m           \"`filenames` must be a `tf.data.Dataset` of `tf.string` elements.\")\n\u001b[0m\u001b[0;32m     50\u001b[0m     if not dataset_ops.get_legacy_output_shapes(filenames).is_compatible_with(\n\u001b[0;32m     51\u001b[0m         tensor_shape.TensorShape([])):\n",
      "\u001b[1;31mTypeError\u001b[0m: `filenames` must be a `tf.data.Dataset` of `tf.string` elements."
     ]
    }
   ],
   "source": [
    "models = [] \n",
    "oof_image_name = [] \n",
    "oof_target = [] \n",
    "oof_prediction = [] \n",
    "\n",
    "i = 1 \n",
    "\n",
    "for train_idx,valid_idx,training_path,validation_path in Fold.create_tfrecord_fold_generator():\n",
    "\n",
    "    tf.keras.backend.clear_session() # Clear session so the other model will be trained on new one.\n",
    "\n",
    "    print(\"=\"* 20,f'Fold_{i}',\"=\" * 20)\n",
    "    \n",
    "    save_path = f'./model/saved_models/EfficientNetB3_Non_Linear_LS_0.05_fold_{i}.h5'\n",
    "\n",
    "    Data = Dataset(\n",
    "        train_files = training_path,\n",
    "        test_files = TEST_TFRECORDS,\n",
    "        validation_files = validation_path,\n",
    "        validation_split = VALIDATION_SPLIT,\n",
    "        image_size = IMAGE_SIZE,\n",
    "        shuffle = SHUFFLE, \n",
    "        dataset_size = TRAINING_DATA_SIZE,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        resize_shape = RESIZE_SHAPE,\n",
    "        include_meta = True\n",
    "    )\n",
    "\n",
    "    Model = ModelCreation(\n",
    "        architecture = 'efficientnet',\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        input_shape = INPUT_SHAPES,\n",
    "        output_shape = 1, \n",
    "        optimizer = Adam,\n",
    "        metric = [BinaryAccuracy(),AUC()],\n",
    "        loss = BinaryCrossentropy(label_smoothing=0.05),\n",
    "        mode = 'meta',\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    training = Data.get_training_dataset()\n",
    "    validation = Data.get_validation_dataset()\n",
    "\n",
    "    callbacks = [ \n",
    "        ModelCheckpoint(filepath=save_path,monitor='val_auc',verbose=1,save_best_only=True,mode='max'),\n",
    "        LearningRateScheduler(scheduler,verbose=1),\n",
    "                ]\n",
    "    \n",
    "    history = Model.model.fit(training,steps_per_epoch = Data.get_train_steps_per_epoch(),\n",
    "                        epochs = EPOCHS,callbacks = Model.inject_callbacks(callbacks),\n",
    "                        validation_data = validation,verbose = 1)\n",
    "    \n",
    "    models.append(Model.model)\n",
    "\n",
    "    dataset_full = Data.get_full_dataset(validation_path)\n",
    "\n",
    "    image_name = dataset_full.map(lambda image,image_name,target: image_name).unbatch() \n",
    "    image_name = next(iter(image_name.batch(len(val_idx)))).numpy().astype('U')\n",
    "\n",
    "    target = dataset_full.map(lambda image,image_name,target : target).unbatch()\n",
    "    target = next(iter(target.batch(len(val_idx)))).numpy()\n",
    "\n",
    "    image = dataset_full.map(lambda image,image_name,target : image)\n",
    "\n",
    "    Load_model = load_model(save_path)\n",
    "\n",
    "    probabilites = Load_model.predict(image)\n",
    "\n",
    "    oof_image_name.extend(list(image_name))\n",
    "\n",
    "    oof_target.extend(list(target))\n",
    "\n",
    "    oof_prediction.extend(list(np.concatenate(probabilities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}