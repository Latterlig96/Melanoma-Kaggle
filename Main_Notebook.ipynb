{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.metrics import BinaryAccuracy,AUC\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,LearningRateScheduler\n",
    "from utils import build_lrfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 \n",
    "EPOCHS = 5\n",
    "TRAINING_DATA_SIZE = 33126\n",
    "TEST_DATA_SIZE = 10982\n",
    "IMAGE_SIZE = [512,512]\n",
    "RESIZE_SHAPE = [512,512]\n",
    "VALIDATION_SPLIT = 0.2\n",
    "N_SPLITS = 5\n",
    "LEARNING_RATE = 1e-5\n",
    "LR_MAX = 0.0004 \n",
    "LR_MIN = 1e-6 \n",
    "LR_RAMPUP_EPOCHS = 5\n",
    "LR_SUSTAIN_EPOCHS = 1\n",
    "LR_EXP_DECAY = 0.8\n",
    "scheduler = build_lrfn(lr_start=LEARNING_RATE,\n",
    "                       lr_max=LR_MAX,\n",
    "                       lr_min=LR_MIN,\n",
    "                       lr_rampup_epochs=LR_RAMPUP_EPOCHS,\n",
    "                       lr_sustain_epochs=0,\n",
    "                       lr_exp_decay=LR_EXP_DECAY)\n",
    "test = tf.io.gfile.glob('./Dataset/tfrecords/test*.tfrec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_val_predictions = np.zeros((TRAINING_DATA_SIZE,))\n",
    "test_preds = np.zeros((TEST_DATA_SIZE,N_SPLITS))\n",
    "test_idnums = np.zeros((TEST_DATA_SIZE,))\n",
    "\n",
    "\n",
    "\n",
    "for i in range(N_SPLITS):\n",
    "\n",
    "    training_file = tf.io.gfile.glob('train*.tfrec')[i]\n",
    "    validation_file = tf.io.gfile.glob('validation*.tfrec')[i]\n",
    "\n",
    "    save_path = f'./model/saved_models/EfficientNetB5_fold_{i+1}.h5'\n",
    "\n",
    "    print(f\"Save model in path: {save_path}\")\n",
    "\n",
    "    callbacks = [ \n",
    "        ModelCheckpoint(save_path,monitor='val_auc',verbose=1,save_best_only=True),\n",
    "        LearningRateScheduler(scheduler,verbose=1)\n",
    "    ]\n",
    "\n",
    "    Data = Dataset(\n",
    "        train_files = training_file,\n",
    "        test_files = test,\n",
    "        validation_files = validation_file,\n",
    "        validation_split = VALIDATION_SPLIT,\n",
    "        image_size = IMAGE_SIZE,\n",
    "        shuffle = SHUFFLE,\n",
    "        dataset_size = TRAINING_DATA_SIZE,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        resize_shape = RESIZE_SHAPE\n",
    "    )\n",
    "\n",
    "    Model = ModelCreation(\n",
    "        architecture = 'efficientnet',\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        input_shape = (*RESIZE_SHAPE,3),\n",
    "        output_shape = 1, \n",
    "        optimizer = Adam,\n",
    "        metric = [BinaryAccuracy(),AUC()],\n",
    "        loss = BinaryCrossentropy,\n",
    "        linear = True,\n",
    "        verbose = True\n",
    "    )\n",
    "\n",
    "    training = Data.get_training_dataset()\n",
    "    validation = Data.get_validation_dataset()\n",
    "\n",
    "\n",
    "    history = Model.model.fit(training,\n",
    "                            steps_per_epoch = Data.get_train_steps_per_epoch(),\n",
    "                            epochs = EPOCHS,\n",
    "                            validation_data = validation,\n",
    "                            validation_steps = Data.get_validation_steps_per_epoch(),\n",
    "                            verbose = 1,\n",
    "                            callbacks = Model.inject_callbacks(callbacks))\n",
    "\n",
    "    print(f\"Loading model from path: {save_path}\")\n",
    "\n",
    "    Load_model = load_model(save_path)\n",
    "\n",
    "    validation_images = validation_data.map(lambda image,label: image)\n",
    "    \n",
    "    probabilities = Load_model.predict(validation_images)\n",
    "\n",
    "    oof_val_predictions[val_idx] = np.concatenate(probabilities)\n",
    "\n",
    "    #Get test set predictions in fold \n",
    "\n",
    "    test_data = Data.get_test_dataset()\n",
    "\n",
    "    test_images = test_data.map(lambda image,idnum : image)\n",
    "    test_probabilities = Load_model.predict(test_images)\n",
    "\n",
    "    test_preds[:,i] = np.concatenate([test_probabilities])\n",
    "\n",
    "    test_ids = test_data.map(lambda image,idnum: idnum).unbatch()\n",
    "    test_ids_images = next(iter(test_ids.batch(TEST_DATA_SIZE))).numpy().astype('U')\n",
    "\n",
    "    test_idnums[:] = test_ids_images\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}